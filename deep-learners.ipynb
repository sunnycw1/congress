{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-words,-fully-connected-with-1-hidden-layer\" data-toc-modified-id=\"Bag-of-words,-fully-connected-with-1-hidden-layer-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Bag-of-words, fully connected with 1 hidden layer</a></span><ul class=\"toc-item\"><li><span><a href=\"#baseline-sgdclassifier\" data-toc-modified-id=\"baseline-sgdclassifier-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>baseline sgdclassifier</a></span></li><li><span><a href=\"#keras-with-hidden-layer\" data-toc-modified-id=\"keras-with-hidden-layer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>keras with hidden layer</a></span></li><li><span><a href=\"#CountVectorizer\" data-toc-modified-id=\"CountVectorizer-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>CountVectorizer</a></span></li></ul></li><li><span><a href=\"#Word-embedding,-fully-connected\" data-toc-modified-id=\"Word-embedding,-fully-connected-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word embedding, fully connected</a></span></li><li><span><a href=\"#CNN---global-max-pooling\" data-toc-modified-id=\"CNN---global-max-pooling-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>CNN - global max pooling</a></span><ul class=\"toc-item\"><li><span><a href=\"#dropout\" data-toc-modified-id=\"dropout-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>dropout</a></span></li></ul></li><li><span><a href=\"#CNN-with-window\" data-toc-modified-id=\"CNN-with-window-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CNN with window</a></span></li><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>RNN</a></span></li><li><span><a href=\"#Bert-Transformer\" data-toc-modified-id=\"Bert-Transformer-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Bert Transformer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plan\n",
    "- one-hot encoding + fully connected network\n",
    "- word embedding + fully connected network\n",
    "- convolutional neural network over whole sentence\n",
    "- convolutional neural network with window\n",
    "- recurrent neural network\n",
    "- bert transformer\n",
    "\n",
    "~1,000,000 parameters worked well with linear SVM. Gives approx lower bound on number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills = pd.read_csv('US-Legislative-congressional_bills_18.1.csv', \n",
    "                    usecols=['description','majortopic'])\n",
    "bills.dropna(inplace=True)\n",
    "\n",
    "bills['majortopic'] = bills['majortopic'].astype(int)\n",
    "bills = bills[(bills['majortopic'] != 99) & (bills['majortopic']!=23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nconvert_keys = sorted(bills['majortopic'].unique())\n",
    "nconvert_values = range(len(nconvert_keys))\n",
    "dict_nconvert = dict(zip(nconvert_keys, nconvert_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_number_topic = {1: 'Macroeconomics',\n",
    "                     2: 'Civil Rights',\n",
    "                     3: 'Health',\n",
    "                     4: 'Agriculture',\n",
    "                     5: 'Labor',\n",
    "                     6: 'Education',\n",
    "                     7: 'Environment',\n",
    "                     8: 'Energy',\n",
    "                     9: 'Immigration',\n",
    "                     10: 'Transportation',\n",
    "                     12: 'Law and Crime',\n",
    "                     13: 'Social Welfare',\n",
    "                     14: 'Housing',\n",
    "                     15: 'Domestic Commerce',\n",
    "                     16: 'Defense',\n",
    "                     17: 'Technology',\n",
    "                     18: 'Foreign Trade',\n",
    "                     19: 'International Affairs',\n",
    "                     20: 'Government Operations',\n",
    "                     21: 'Public Lands'\n",
    "                    }\n",
    "\n",
    "len(dict_number_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills['topic0'] = bills['majortopic'].map(dict_nconvert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>majortopic</th>\n",
       "      <th>topic0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To increase the rates of certain educational and readjustment allowances payable to veterans in order to compensate for the higher cost of living in Alaska</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   description  \\\n",
       "3  To increase the rates of certain educational and readjustment allowances payable to veterans in order to compensate for the higher cost of living in Alaska   \n",
       "\n",
       "   majortopic  topic0  \n",
       "3  6           5       "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bills.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize_doc(doc, complete=False):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    \n",
    "    # filter stopwords out of document\n",
    "    if complete:\n",
    "        return tokens\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "bills['tokens'] = bills['description'].apply(tokenize_doc)\n",
    "bills['tokens_complete'] = bills['description'].apply(tokenize_doc, complete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create decription from filtered tokens\n",
    "bills['norm_description'] = bills['tokens'].str.join(' ')\n",
    "bills['norm_description_complete'] = bills['tokens_complete'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels = bills[\"topic0\"]\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    bills.drop(columns='topic0'),\n",
    "    list_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=list_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "single_split_cv = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, val_index in single_split_cv.split(X_train_val, y_train_val):\n",
    "    X_train, y_train = X_train_val.iloc[train_index], y_train_val.iloc[train_index]\n",
    "    X_val, y_val = X_train_val.iloc[val_index], y_train_val.iloc[val_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words, fully connected with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'norm_description'\n",
    "train_val = X_train_val[description]\n",
    "train = X_train[description]\n",
    "val = X_val[description]\n",
    "test = X_test[description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(ngram_range=(1,2), n_features=2**18)\n",
    "train_v = vectorizer.fit_transform(train)\n",
    "val_v = vectorizer.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246016, 262144)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline sgdclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(max_iter=5, tol=None, random_state=42, alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(train_v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7979508711993613"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_predicted = sgd.predict(val_v)\n",
    "f1_score(y_val, y_val_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras with hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246016, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = train_v.shape[1]\n",
    "input_shape = (n_cols,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=input_shape))\n",
    "model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 20)                5242900   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "=================================================================\n",
      "Total params: 5,243,320\n",
      "Trainable params: 5,243,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246016 samples, validate on 61504 samples\n",
      "Epoch 1/4\n",
      "246016/246016 [==============================] - 627s 3ms/step - loss: 0.8251 - acc: 0.7895 - val_loss: 0.4996 - val_acc: 0.8610\n",
      "Epoch 2/4\n",
      "246016/246016 [==============================] - 623s 3ms/step - loss: 0.3406 - acc: 0.9021 - val_loss: 0.4680 - val_acc: 0.8708\n",
      "Epoch 3/4\n",
      "246016/246016 [==============================] - 620s 3ms/step - loss: 0.2416 - acc: 0.9262 - val_loss: 0.4833 - val_acc: 0.8710\n",
      "Epoch 4/4\n",
      "246016/246016 [==============================] - 624s 3ms/step - loss: 0.1928 - acc: 0.9377 - val_loss: 0.5112 - val_acc: 0.8690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23eec8b5908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_v,\n",
    "          y_train_cat, \n",
    "          validation_data=(val_v, y_val_cat),\n",
    "          batch_size=32, \n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = model.predict(val_v)\n",
    "y_val_predict = y_val_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8641545611452752"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_predict, y_val_cat.argmax(axis=-1), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvectorizer = CountVectorizer(ngram_range=(1,2), max_features=2**18)\n",
    "train_v = cvectorizer.fit_transform(train)\n",
    "val_v = cvectorizer.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols = train_v.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=input_shape))\n",
    "model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246016 samples, validate on 61504 samples\n",
      "Epoch 1/1\n",
      "246016/246016 [==============================] - 628s 3ms/step - loss: 0.7077 - acc: 0.8277 - val_loss: 0.5145 - val_acc: 0.8653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2410eb45a20>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_v,\n",
    "          y_train_cat, \n",
    "          validation_data=(val_v, y_val_cat),\n",
    "          batch_size=32, \n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.860126142356495"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_prob = model.predict(val_v)\n",
    "y_val_predict = y_val_prob.argmax(axis=-1)\n",
    "f1_score(y_val_predict, y_val_cat.argmax(axis=-1), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding, fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\congress\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_saved_word2vec.txt') as f:\n",
    "    path = Path(f.readline())\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = path / \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "if 'word2vec' not in locals():\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6892118 words total, with a vocabulary size of 39391\n",
      "Max sentence length is 20\n"
     ]
    }
   ],
   "source": [
    "bills[\"tokens20\"] = bills[\"tokens_complete\"].apply(lambda x: x[:20])\n",
    "all_words_complete = [word for tokens in bills[\"tokens20\"] for word in tokens]\n",
    "VOCAB_COMPLETE = sorted(list(set(all_words_complete)))\n",
    "sentence_lengths = [len(tokens) for tokens in bills[\"tokens20\"]]\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words_complete), len(VOCAB_COMPLETE)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "VOCAB_SIZE = len(VOCAB_COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49622 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() #num_words=VOCAB_SIZE\n",
    "tokenizer.fit_on_texts(bills[\"description\"])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_k = tokenizer.texts_to_sequences(X_train[\"description\"])\n",
    "val_k = tokenizer.texts_to_sequences(X_val[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49623, 300)\n"
     ]
    }
   ],
   "source": [
    "# rows of embedding_weights are vector embedding for each of the words in word_index\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((246016, 20), (246016, 20))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pad_sequences(train_k, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val = pad_sequences(val_k\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    , maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "train.shape, y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 300)           14886900  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                120020    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                420       \n",
      "=================================================================\n",
      "Total params: 15,007,340\n",
      "Trainable params: 15,007,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246016 samples, validate on 61504 samples\n",
      "Epoch 1/10\n",
      "246016/246016 [==============================] - 36s 148us/step - loss: 1.1932 - acc: 0.6727 - val_loss: 0.8048 - val_acc: 0.7867\n",
      "Epoch 2/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.6180 - acc: 0.8337 - val_loss: 0.7156 - val_acc: 0.8104\n",
      "Epoch 3/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.4466 - acc: 0.8760 - val_loss: 0.7146 - val_acc: 0.8202\n",
      "Epoch 4/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.3390 - acc: 0.9036 - val_loss: 0.7537 - val_acc: 0.8204\n",
      "Epoch 5/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.2745 - acc: 0.9201 - val_loss: 0.7897 - val_acc: 0.8218\n",
      "Epoch 6/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.2375 - acc: 0.9304 - val_loss: 0.8101 - val_acc: 0.8226\n",
      "Epoch 7/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.2149 - acc: 0.9367 - val_loss: 0.8494 - val_acc: 0.8192\n",
      "Epoch 8/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.2008 - acc: 0.9399 - val_loss: 0.8400 - val_acc: 0.8226\n",
      "Epoch 9/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.1888 - acc: 0.9425 - val_loss: 0.8494 - val_acc: 0.8226\n",
      "Epoch 10/10\n",
      "246016/246016 [==============================] - 36s 145us/step - loss: 0.1796 - acc: 0.9447 - val_loss: 0.8473 - val_acc: 0.8242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23eeba47748>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,\n",
    "          y_train_cat, \n",
    "          validation_data=(val, y_val_cat),\n",
    "          batch_size=128, \n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - global max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1408.5882.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in train_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pad_sequences(train_k, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val = pad_sequences(val_k, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246016, 275)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_in = Input(shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\congress\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Conv1D(num_filters, fsz, activation='relu')(graph_in)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    convs.append(pool)\n",
    "\n",
    "out = Concatenate()(convs)\n",
    "graph = Model(input=graph_in, output=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(graph)\n",
    "model.add(Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 275, 300)          14886900  \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 300)               360300    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                6020      \n",
      "=================================================================\n",
      "Total params: 15,253,220\n",
      "Trainable params: 366,320\n",
      "Non-trainable params: 14,886,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246016 samples, validate on 61504 samples\n",
      "Epoch 1/5\n",
      "246016/246016 [==============================] - 63s 256us/step - loss: 0.8165 - acc: 0.7623 - val_loss: 0.5992 - val_acc: 0.8211\n",
      "Epoch 2/5\n",
      "246016/246016 [==============================] - 63s 255us/step - loss: 0.5332 - acc: 0.8403 - val_loss: 0.5359 - val_acc: 0.8397\n",
      "Epoch 3/5\n",
      "246016/246016 [==============================] - 63s 255us/step - loss: 0.4579 - acc: 0.8613 - val_loss: 0.5228 - val_acc: 0.8439\n",
      "Epoch 4/5\n",
      "246016/246016 [==============================] - 63s 255us/step - loss: 0.4068 - acc: 0.8747 - val_loss: 0.5158 - val_acc: 0.8489\n",
      "Epoch 5/5\n",
      "246016/246016 [==============================] - 63s 257us/step - loss: 0.3685 - acc: 0.8858 - val_loss: 0.5153 - val_acc: 0.8517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23ef8901630>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,\n",
    "          y_train_cat, \n",
    "          validation_data=(val, y_val_cat),\n",
    "          batch_size=128, \n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(graph)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246016 samples, validate on 61504 samples\n",
      "Epoch 1/5\n",
      "246016/246016 [==============================] - 63s 258us/step - loss: 0.8742 - acc: 0.7547 - val_loss: 0.5623 - val_acc: 0.8401\n",
      "Epoch 2/5\n",
      "246016/246016 [==============================] - 63s 257us/step - loss: 0.6403 - acc: 0.8195 - val_loss: 0.5383 - val_acc: 0.8475\n",
      "Epoch 3/5\n",
      "246016/246016 [==============================] - 64s 258us/step - loss: 0.6042 - acc: 0.8302 - val_loss: 0.5389 - val_acc: 0.8497\n",
      "Epoch 4/5\n",
      "246016/246016 [==============================] - 63s 257us/step - loss: 0.5818 - acc: 0.8364 - val_loss: 0.5244 - val_acc: 0.8530\n",
      "Epoch 5/5\n",
      "246016/246016 [==============================] - 63s 256us/step - loss: 0.5643 - acc: 0.8412 - val_loss: 0.5216 - val_acc: 0.8525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x240fe356a58>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,\n",
    "          y_train_cat, \n",
    "          validation_data=(val, y_val_cat),\n",
    "          batch_size=128, \n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
